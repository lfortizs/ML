{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"E12 - Gradient Boosting Review.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Hj14q93fHKZv","colab_type":"text"},"cell_type":"markdown","source":["# ** Gradient Boosting Review**\n","\n","\n","---\n","\n","\n","**Gradient Boosting Classifier**\n","\n","Gradient Boosting es un algoritmo de Machine Learning que busca crear un estimador robusto a partir de estimadores débiles (generlamente decision trees) desde un enfoque de optimización donde se toma la función de pérdida y se optimiza iterativamente el coste en función del error de la estimación. El concepto fue inicialmente desarrollado por Leo Breiman quien hizo un gran aporte a los algoritmos de árboles de decisión y regresión. En Gradient Boosting se inicia el proceso con un estimador débil y se incorpora en cada iteración otro estimador débil que debe mejorar el desempeño del estimador original al reducir la pérdida de la función de pérdida. La pérdida representa el error residual (la diferencia entre el valor real y la predicción) y se usa esta pérdida para actualizar las predicciones incorporando nuevos estimadores débiles y concentrándolos en las áreas donde los estimadores existentes tuvieron un mal desempeño. Este proceso se repite tantas veces hasta que el error tienda a cero.\n","\n","\n","**XGBoost (Extreme Gradient Boosting)**\n","\n","XGBoost es una implementación de Gradient Boosting con mejor desempeño y velocidad que ha tenido mucho éxito en las competencias de Machine Learning con datos tabulares o estructurados. La librería incorpora parámetros de optimización que incluyen variables tanto para el modelo como para el sistema en el cual se corre lo que permite sacar el mayor provecho de los recursos de procesamiento y memoria al momento de entrenar y ejecutar el modelo. Dentro de los parámetros para la ejecución del XGBoost está el número de núcleos de CPU que se pueden usar, implementación en ambientes de computación distribuida (Hadoop),* Out-of-Core Computing*  para conjuntos de datos muy grandes y optimización de memoria caché.\n","\n","Dentro de los parámetros del modelo están:\n","\n","* Penalización inteligente de los árboles\n","* Reducción proporcional de los nodos terminales\n","* Parámetro extra de aleatoriedad\n","\n","\n"]}]}