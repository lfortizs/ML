# E8 - Ensemble Trees Overview

En Machine Learing el propósito de los métodos de ensamblaje es combinar diferentes árboles de decisión para mejorar las métricas de predicción que se puedan obtener al usar un único árbol de decisión. El principio detrás de este método es que la suma de diferentes predictores débiles al combinarse o ensamblarse producirán una predicción con mayor precisión.
Dentro de las técnicas para realizar el ensamblaje se encuentran:
Bagging: La idea principal de es poder generar un nuevo subconjunto de datos a partir de la muestra para training escogiendo con reemplazo y de manera aleatoria los registros de ese subset. A partir de cada colección de datos de la muestra se entrenará cada árbol de decisión. Como resultado se obtiene un ensamblaje de modelos entrenados con un subset de datos de entrenamiento que pueden contener diferentes versiones del set de entrenamiento. El promedio de las predicciones de los diferentes árboles resultará en un modelo más robusto que la predicción de un solo árbol de decisión. 
Random Forest es una extensión de Baggin donde se agrega un paso extra, donde además de seleccionar de manera aleatoria el subset de datos de entrenamiento se toma de forma aleatoria una selección de las variables o features en lugar de usar todas las variables del set original. El resultado es múltiples árboles entrenados con subset de datos aleatorios y diferentes combinaciones de variables. La ventaja de este algoritmo es que tiene buen desempeño ante altos volúmenes de datos y la precisión no se ve afectada ante datasets que contengan muchos datos faltantes. 
Boosting es otra técnica de ensamblaje para crear un conjunto de predictores que reduzca el sesgo y la varianza . Existe Bosting para catorias binarias y boosting para categorización multiclase
